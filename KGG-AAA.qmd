---
title: Query checker
jupyter:
  jupytext:
    formats: 'ipynb,qmd'
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
with open('/home/asilva/quarto/2024-PyData-Global/assets/curie.txt', "r") as f:
    curie_text = f.read()
```

```{python}
texts = curie_text.split("\n\n") # chunking
```

```{python}
from pydantic.v1 import BaseModel, Field
from typing import Optional

class Node(BaseModel):
    """Node of the Knowledge Graph"""
    id: int = Field(..., description="Unique identifier of the node")
    type: str = Field(..., description="Type of the node")
    label: str = Field(..., description="Label of the node")
    property: Optional[str] = Field(default=None, description="Property of the node")

class Edge(BaseModel):
    """Edge of the Knowledge Graph"""
    source: int = Field(..., description="Unique source of the edge")
    target: int = Field(..., description="Unique target of the edge")
    type: str = Field(..., description="Type of the edge")
    label: str = Field(..., description="Label of the edge")
    property: Optional[str] = Field(default=None, description="Property of the edge")
```

```{python}
from typing import List

class KnowledgeGraph(BaseModel):
    """Generated Knowledge Graph"""
    nodes: List[Node] = Field(..., description="List of nodes of the knowledge graph")
    edges: List[Edge] = Field(..., description="List of edges of the knowledge graph")
```

```{python}
json_schema = KnowledgeGraph.schema_json() # json_schema = KnowledgeGraph.model_json_schema()
```

```{python}
from outlines.fsm.json_schema import convert_json_schema_to_str
from outlines.fsm.json_schema import build_regex_from_schema

schema_str = convert_json_schema_to_str(json_schema=json_schema)
regex_str = build_regex_from_schema(schema_str)
```

```{python}
import llama_cpp

llm = llama_cpp.Llama(
    "/big_storage/llms/models/Hermes-3-Llama-3.1-8B.Q6_K.gguf",
    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(
        "NousResearch/Hermes-3-Llama-3.1-8B"
    ),
    n_gpu_layers=-1,
    flash_attn=True,
    n_ctx=8192,
    verbose=False
)
```

```{python}
import outlines
import transformers

outlines_tokenizer = outlines.models.TransformerTokenizer(
    transformers.AutoTokenizer.from_pretrained("NousResearch/Hermes-3-Llama-3.1-8B")
)
```

```{python}
def generate_hermes_prompt(user_prompt):
    return (
        "<|im_start|>system\n"
        "You are a world class AI model who answers questions in JSON with correct Pydantic schema. "
        f"Here's the json schema you must adhere to:\n<schema>\n{json_schema}\n</schema><|im_end|>\n"
        "<|im_start|>user\n"
        + "Describe the following text as a detailed knowledge graph in JSON:\n"+ user_prompt
        + "<|im_end|>"
        + "\n<|im_start|>assistant\n"
        "<schema>"
    )
```

```{python}
import json

nodes = []
edges = []
for text in texts:
    nodes_graph = []
    edges_graph = []
    prompt = generate_hermes_prompt(text)
    outlines_logits_processor = outlines.processors.RegexLogitsProcessor(
        regex_str,
        outlines_tokenizer,
    )
    output = llm.create_completion(
        prompt,
        logits_processor=transformers.LogitsProcessorList([outlines_logits_processor]),
        max_tokens=1000
    )
    prompt += output['choices'][0]['text'] + "</schema><|im_end|>\n<|im_start|>user\nCorrect the generated knowledge graph and add the missing details.<|im_end|>\n<|im_start|>assistant\n<schema>"
    outlines_logits_processor = outlines.processors.RegexLogitsProcessor(
        regex_str,
        outlines_tokenizer,
    )
    output = llm.create_completion(
        prompt,
        logits_processor=transformers.LogitsProcessorList([outlines_logits_processor]),
        max_tokens=1000
    )
    for node in json.loads(output['choices'][0]['text'])['nodes']:
        nodes_graph.append(node)
    for edge in json.loads(output['choices'][0]['text'])['edges']:
        edges_graph.append(edge)
    nodes.append(nodes_graph)
    edges.append(edges_graph)
```

```{python}
from collections import Counter

print(Counter([node['type'].upper() for i in range(len(texts)) for node in nodes[i]])),
print(Counter([edge['type'].upper() for i in range(len(texts)) for edge in edges[i]]))
```

```{python}
from enum import Enum
from pydantic.v1 import BaseModel, Field
from typing import Literal

class Node(BaseModel):
    """Node of the Knowledge Graph"""
    id: int = Field(..., description="Unique identifier of the node starting from 0.")
    type : Literal["PERSON", "AWARD", "DISCOVERY", "LOCATION", "OTHER"] = Field(..., description="Type of the node")
    label: str = Field(..., description="Label of the node")
    property: Optional[str] = Field(default=None, description="Property of the node")

class Edge(BaseModel):
    """Edge of the Knowledge Graph"""
    source: int = Field(..., description="Unique source of the edge")
    target: int = Field(..., description="Unique target of the edge")
    label: str = Field(..., description="Label of the edge")
    type: Literal["DISCOVERED", "AWARDED", "INTERPERSONAL_RELATIONSHIP", "VISITED", "OTHER"] = Field(..., description="Type of the edge")
    property: Optional[str] = Field(default=None, description="Property of the edge")
```

```{python}
from typing import List

class KnowledgeGraph(BaseModel):
    """Generated Knowledge Graph"""
    nodes: List[Node] = Field(..., description="List of nodes of the knowledge graph")
    edges: List[Edge] = Field(..., description="List of edges of the knowledge graph")
```

```{python}
json_schema = KnowledgeGraph.schema_json()
```

```{python}
from outlines.fsm.json_schema import convert_json_schema_to_str
from outlines.fsm.json_schema import build_regex_from_schema

schema_str = convert_json_schema_to_str(json_schema=json_schema)
regex_str = build_regex_from_schema(schema_str)
```

```{python}
nodes = []
edges = []
for text in texts:
    nodes_graph = []
    edges_graph = []
    prompt = generate_hermes_prompt(text)
    outlines_logits_processor = outlines.processors.RegexLogitsProcessor(
        regex_str,
        outlines_tokenizer,
    )
    output = llm.create_completion(
        prompt,
        temperature=.1,
        logits_processor=transformers.LogitsProcessorList([outlines_logits_processor]),
        max_tokens=1000,
        seed=42
    )
    prompt += output['choices'][0]['text'] + "</schema><|im_end|>\n<|im_start|>user\nCorrect the generated knowledge graph and add the missing details.<|im_end|>\n<|im_start|>assistant\n<schema>"
    outlines_logits_processor = outlines.processors.RegexLogitsProcessor(
        regex_str,
        outlines_tokenizer,
    )
    output = llm.create_completion(
        prompt,
        temperature=.1,
        logits_processor=transformers.LogitsProcessorList([outlines_logits_processor]),
        max_tokens=1000,
        seed=42
    )
    for node in json.loads(output['choices'][0]['text'])['nodes']:
        nodes_graph.append(node)
    for edge in json.loads(output['choices'][0]['text'])['edges']:
        edges_graph.append(edge)
    nodes.append(nodes_graph)
    edges.append(edges_graph)
```

```{python}
nodes[0]
```

```{python}
edges[0]
```

```{python}
from graphviz import Digraph

dot = Digraph()
i = 1
for node in nodes[i]:
    if node['type'] != "OTHER":
        dot.node(str(node['id']), node['type']+"\n"+node['label'], shape='circle', width='1', height='1')
for edge in edges[i]:
    if edge["type"] != "OTHER" and nodes[i][edge["source"]]["type"]!= "OTHER" and nodes[i][edge["target"]]["type"]!= "OTHER":
        dot.edge(str(edge['source']), str(edge['target']), label=edge['type']+"\n"+ edge['label'])
dot
```

```{python}
import pandas as pd

df_nodes = pd.DataFrame({
    'id': [node['id'] for node in nodes[0]],
    'label': [node['label'] for node in nodes[0]],
    'type': [node['type'] for node in nodes[0]],
    'property': [node['property'] for node in nodes[0]]
})
```

```{python}
df_edges = pd.DataFrame({
    'source': [nodes[0][edge['source']]['label'] for edge in edges[0]],
    'target': [nodes[0][edge['target']]['label'] for edge in edges[0]],
    'type': [edge['type'] for edge in edges[0]],
    'label': [edge['label'] for edge in edges[0]],
    'property': [edge['property'] for edge in edges[0]]
})
```

```{python}
for i in range(1,len(texts)):
    df_nodes_aux = pd.DataFrame({
        'id': [node['id']+len(df_nodes) for node in nodes[i]],
        'label': [node['label'] for node in nodes[i]],
        'type': [node['type'] for node in nodes[i]],
        'property': [node['property'] for node in nodes[i]]
    })
    df_edges_aux = pd.DataFrame({
        'source': [nodes[i][edge['source']]['label'] for edge in edges[i]],
        'target': [nodes[i][edge['target']]['label'] for edge in edges[i]],
        'type': [edge['type'] for edge in edges[i]],
        'label': [edge['label'] for edge in edges[i]],
        'property': [edge['property'] for edge in edges[i]]
    })
    df_nodes = pd.concat([df_nodes, df_nodes_aux])
    df_edges = pd.concat([df_edges, df_edges_aux])
```

```{python}
df_nodes = df_nodes[df_nodes["type"] != "OTHER"].drop_duplicates(subset="label")[["label", "type", "property"]]
df_edges = df_edges[df_edges["type"] != "OTHER"]
```

```{python}
df_nodes_person = df_nodes[df_nodes["type"] == "PERSON"][["label"]]
df_nodes_award = df_nodes[df_nodes["type"] == "AWARD"][["label"]]
df_nodes_discovery = df_nodes[df_nodes["type"] == "DISCOVERY"][["label"]]
df_nodes_location = df_nodes[df_nodes["type"] == "LOCATION"][["label"]]
```

```{python}
df_nodes_person
```

```{python}
def my_func(edge_type: str, source: str, target: str):
    mask1 = [a in list(df_nodes[df_nodes["type"] == source]['label']) for a in df_edges[df_edges["type"] == edge_type]["source"]]
    mask2 = [a in list(df_nodes[df_nodes["type"] == target]['label']) for a in df_edges[df_edges["type"] == edge_type]["target"]]
    mask = [a & b for a,b in zip(mask1,mask2)]
    return df_edges[df_edges["type"] == edge_type][mask]
```

```{python}
df_edges_awarded = my_func("AWARDED", "PERSON", "AWARD")[["source", "target"]]
df_edges_discovered = my_func("DISCOVERED", "PERSON", "DISCOVERY")[["source", "target"]]
df_edges_visited = my_func("VISITED", "PERSON", "LOCATION")[["source", "target"]]
df_edges_interpersonal_relationship = my_func("INTERPERSONAL_RELATIONSHIP", "PERSON", "PERSON")[["source", "target"]]
```

```{python}
df_edges_discovered
```

```{python}
import kuzu

db = kuzu.Database()
conn = kuzu.Connection(db)
conn.execute("CREATE NODE TABLE Person(name STRING, PRIMARY KEY (name))");
conn.execute("CREATE NODE TABLE Award(name STRING, PRIMARY KEY (name))");
conn.execute("CREATE NODE TABLE Discovery(name STRING, PRIMARY KEY (name))");
conn.execute("CREATE NODE TABLE Location(name STRING, PRIMARY KEY (name))");
conn.execute("COPY Person FROM df_nodes_person");
conn.execute("COPY Award FROM df_nodes_award");
conn.execute("COPY Discovery FROM df_nodes_discovery");
conn.execute("COPY Location FROM df_nodes_location");
```

```{python}
conn.execute("CREATE REL TABLE Awarded(FROM Person TO Award)");
conn.execute("CREATE REL TABLE Discovered(FROM Person TO Discovery)");
conn.execute("CREATE REL TABLE Interpersonal_Relationship(FROM Person TO Person)");
conn.execute("CREATE REL TABLE Visited(FROM Person TO Location)");
conn.execute("COPY Discovered FROM df_edges_discovered");
conn.execute("COPY Awarded FROM df_edges_awarded");
conn.execute("COPY Visited FROM df_edges_visited");
conn.execute("COPY Interpersonal_Relationship FROM df_edges_interpersonal_relationship");
```

```{python}
result = conn.execute("MATCH (a)-[b]->(c) RETURN a.name, c.name;")
while result.has_next():
    print(result.get_next())
```

```{python}
from langchain_community.graphs import KuzuGraph

graph_db_schema = KuzuGraph(db).get_schema
print(graph_db_schema)
```

```{python}
print(graph_db_schema)
```

```{python}
def generate_kuzu_prompt(query, graph_db_schema=graph_db_schema):
    return """Task: Generate Kùzu Cypher statement to query a graph database.

Instructions:
Generate the Kùzu dialect of Cypher with the following rules in mind:
1. Do not omit the relationship pattern. Always use `()-[]->()` instead of `()->()`.
2. Do not include triple backticks ``` in your response. Return only Cypher.
3. Do not return any notes or comments in your response.

Use only the provided relationship types and properties in the schema.
Do not use any other relationship types or properties that are not provided.
Schema:\n""" + graph_db_schema + """
\nExample:
The question is:\n"Which songs does the load album have?"
MATCH (a:ALBUM {name: 'Load'})<-[:IN_ALBUM]-(s:SONG) RETURN s.name
Note: Do not include any explanations or apologies in your responses.
Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.
Do not include any text except the generated Cypher statement.

The question is:\n""" + query
```

```{python}
print(generate_kuzu_prompt("What discovery is Marie Curie famous for?"))
```



```{python}
def generate_kuzu_query_checker_prompt(cypher_query):
    return (
    f"\n{cypher_query}"
    "\nDouble check the Kùzu dialect of Cypher for the query above for common mistakes, "
    "including:\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n"
    "\n- Do not omit the relationship pattern." 
    "\n- Always use `()-[]->()` instead of `()->()`.\n\n"
    "If there are any of the above mistakes, rewrite the query. "
    "If there are no mistakes, just reproduce the original query.\n\n"
    "Output the final Kùzu Cypher query only.\n\nKùzu Cypher Query: "
    )
```

```{python}
def query_checker_tool(cypher_query):
    user_prompt = generate_kuzu_query_checker_prompt(cypher_query)
    response = llm.create_chat_completion(
            messages = [
                {
                    "role": "user",
                    "content": user_prompt
                }
            ]
        )
    return response['choices'][0]['message']['content']
```

```{python}
query_checker_tool("""MATCH (s:SONG)-[:IN_ALBUM]-<(:ALBUM {name: 'The Black Album'})\nRETURN COUNT(s)""")
```

```{python}
def query_generator_tool(query):
    print(query)
    user_prompt = generate_kuzu_prompt(query)
    output = llm.create_chat_completion(
            messages = [
                {
                    "role": "user",
                    "content": user_prompt
                }
            ]
        )
    cypher_query = output['choices'][0]['message']['content']
    print(f"\x1b[33m Cypher query: {cypher_query} \x1b[0m")
    cypher_query = query_checker_tool(cypher_query)
    print(f"\x1b[33m Cypher query: {cypher_query} \x1b[0m")
    response = conn.execute(
            f"""
            {cypher_query}
            """
        );
    df = response.get_as_pl()
    col_name = df.columns[0]
    _values = df[col_name].to_list()
    return f"[{col_name}: {_values}]"
```

```{python}
query_generator_tool("What discovery is Marie Curie famous for?")
```

## Conversational response tool

```{python}
def conversational_response(text):
    # response = llm.create_chat_completion(
    #     messages = [
    #         {
    #             "role": "user",
    #             "content": text
    #         }
    #     ]
    # )
    # return response['choices'][0]['message']['content']
    return text
```

```{python}
conversational_response("""Respond to the user's greeting in a friendly manner.""")
```

## ReAct agent

```{python}
from enum import Enum

class Action(str, Enum):
    conversational_response = "conversational_response"
    query_generator = "query_generator"
    query_checker = "query_checker"
```

```{python}
from pydantic.v1 import BaseModel, Field

class Reason_and_Act(BaseModel):
    Scratchpad: str = Field(..., description="Information from the Observation useful to answer the question")
    Thought: str = Field(..., description="It describes your thoughts about the question you have been asked")
    Action: Literal["conversational_response", "query_generator", "query_checker"] = Field(..., description="The action to take")
    Action_Input: str = Field(..., description="The arguments of the Action.")
```

```{python}
class Final_Answer(BaseModel):
    Scratchpad: str = Field(..., description="Information from the Observation useful to answer the question")
    Final_Answer: str = Field(..., description="Answer to the question grounded on the Observation")
```

```{python}
class Decision(BaseModel):
    Decision: Reason_and_Act | Final_Answer
```

```{python}
Decision.schema_json()
```

```{python}
json_schema = Decision.schema_json()
json_schema
```

```{python}
from outlines.fsm.json_schema import convert_json_schema_to_str
from outlines.fsm.json_schema import build_regex_from_schema

schema_str = convert_json_schema_to_str(json_schema=json_schema)
schema_str
```

```{python}
regex_str = build_regex_from_schema(schema_str)
regex_str
```

```{python}
def generate_hermes_prompt(question, schema=""):
    return (
        "<|im_start|>system\n"
        "You are a world class AI model who answers questions in JSON with correct Pydantic schema."
        f"\nHere's the JSON schema you must adhere to:\n<schema>\n{schema}\n</schema>\n"
        "You run in a loop of Scratchpad, Thought, Action, Action Input, PAUSE, Observation."
        "\nAt the end of the loop you output a Final Answer. "
        "\n- Use Scratchpad to store the information from the observation useful to answer the question"
        "\n- Use Thought to describe your thoughts about the question you have been asked "
        "and reflect carefully about the Observation if it exists. "
        "\n- Use Action to run one of the actions available to you. "
        "\n- Use Action Input to input the arguments of the selected action - then return PAUSE. "
        "\n- Observation will be the result of running those actions. "
        "\nYour available actions are:\n"
        "query_generator:\n" 
        "e.g. query_generator: Who is Marie Curie related to?\n"
        "Returns a detailed and correct Kùzu Cypher query\n"
        "query_checker:\n"
        "e.g. query_checker: MATCH (a:ALBUM {name: 'The Black Album'})<-[:IN_ALBUM]-(s:SONG) RETURN COUNT(s)\n"
        "Returns a detailed and correct Kùzu Cypher query after double checking the query for common mistakes\n"
        "conversational_reponse:"
        "e.g. conversational_response: Hi!\n"
        "Returns a conversational response to the user\n"
        "DO NOT TRY TO GUESS THE ANSWER. Begin! <|im_end|>"
        "\n<|im_start|>user\n" + question + "<|im_end|>"
        "\n<|im_start|>assistant\n"
    )
```

```{python}
print(generate_hermes_prompt("", schema_str))
```

```{python}
from outlines import generate, models
model = models.LlamaCpp(llm)
```

```{python}
class ChatBot:
    def __init__(self, prompt=""):
        self.prompt = prompt

    def __call__(self, user_prompt):
        self.prompt += user_prompt
        # print(self.prompt)
        result = self.execute()
        return result
        
    def execute(self):
        generator = generate.regex(model, regex_str)
        result = generator(self.prompt, max_tokens=1024, temperature=0, seed=42)
        return result
```

```{python}
import json

def query(question, max_turns=5):
    i = 0
    next_prompt = (
        "\n<|im_start|>user\n" + question + "<|im_end|>"
        "\n<|im_start|>assistant\n"
    )
    previous_actions = []
    while i < max_turns:
        i += 1
        prompt = generate_hermes_prompt(question=question, schema=Decision.schema_json())
        bot = ChatBot(prompt=prompt)
        result = bot(next_prompt)
        json_result = json.loads(result)['Decision']
        if "Final_Answer" not in list(json_result.keys()):
            scratchpad = json_result['Scratchpad'] if i == 0 else ""
            thought = json_result['Thought']
            action = json_result['Action']
            action_input = json_result['Action_Input']
            print(f"\x1b[34m Scratchpad: {scratchpad} \x1b[0m")
            print(f"\x1b[34m Thought: {thought} \x1b[0m")
            print(f"\x1b[36m  -- running {action}: {str(action_input)}\x1b[0m")
            if action + ": " + str(action_input) in previous_actions:
                observation = "You already run that action. **TRY A DIFFERENT ACTION INPUT.**"
            else:
                if action=="query_checker":
                    try:
                        observation = query_checker_tool(str(action_input))
                        # observation = eval(str(action_input))
                    except Exception as e:
                        observation = f"{e}"
                elif action=="query_generator":
                    try:
                        observation = query_generator_tool(str(action_input))
                        # observation = wikipedia(str(action_input))
                    except Exception as e:
                        observation = f"{e}"
                elif action=="conversational_response":
                    try:
                        observation = conversational_response(str(action_input))
                        observation += "\nAnswer to the user."
                    except Exception as e:
                        observation = f"{e}"
            print()
            print(f"\x1b[33m Observation: {observation} \x1b[0m")
            print()
            previous_actions.append(action + ": " + str(action_input))
            next_prompt += (
                "\nScratchpad: " + scratchpad +
                "\nThought: " + thought +
                "\nAction: " + action  +
                "\nAction Input: " + action_input +
                "\nObservation: " + str(observation)
            )
        else:
            scratchpad = json_result["Scratchpad"]
            final_answer = json_result["Final_Answer"]
            print(f"\x1b[34m Scratchpad: {scratchpad} \x1b[0m")
            print(f"\x1b[34m Final Answer: {final_answer} \x1b[0m")
            return final_answer
    print(f"\nFinal Answer: I am sorry, but I am unable to answer your question. Please provide more information or a different question.")
    return "No answer found"
```

```{python}
query("Which discovery did Pierre Curie do?")
```

```{python}
query("Which discovery did Marie Curie do?")
```

```{python}
query("Tell me a joke")
```

```{python}
query("Hi! How are you?")
```

